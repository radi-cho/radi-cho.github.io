<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>Radostin Cholakov</title>
    <meta name="author" content="Radostin Cholakov" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, viewport-fit=cover"
    />

    <!-- Google Search / SEO Meta Tags -->
    <meta
      name="description"
      content="Radostin Cholakov - CS student at Stanford University focused on efficient large language models, quantization, novel architectures, reasoning, and representation learning."
    />
    <meta
      name="keywords"
      content="Radostin Cholakov, Stanford University, machine learning, large language models, quantization, AI research, computer science"
    />
    <meta name="robots" content="index, follow" />
    <meta name="googlebot" content="index, follow" />
    <link rel="canonical" href="https://radicho.com/" />

    <!-- Open Graph / Facebook Meta Tags -->
    <meta property="og:title" content="Radostin Cholakov" />
    <meta
      property="og:description"
      content="CS student at Stanford University focused on efficient large language models, quantization, novel architectures, reasoning, and representation learning."
    />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://radicho.com/" />
    <meta
      property="og:image"
      content="https://radicho.com/images/profilepic.jpg"
    />
    <meta property="og:image:width" content="400" />
    <meta property="og:image:height" content="400" />
    <meta property="og:site_name" content="Radostin Cholakov" />
    <meta property="og:locale" content="en_US" />

    <!-- Twitter / X Meta Tags -->
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:site" content="@radi_cho" />
    <meta name="twitter:creator" content="@radi_cho" />
    <meta name="twitter:title" content="Radostin Cholakov" />
    <meta
      name="twitter:description"
      content="CS student at Stanford University focused on efficient large language models, quantization, novel architectures, reasoning, and representation learning."
    />
    <meta
      name="twitter:image"
      content="https://radicho.com/images/profilepic.jpg"
    />
    <meta name="twitter:image:alt" content="Radostin Cholakov profile photo" />

    <!-- ChatGPT / AI Assistant Meta Tags -->
    <meta
      name="ai:description"
      content="Personal website of Radostin Cholakov, a CS student at Stanford University researching efficient large language models, quantization, novel architectures, reasoning, and representation learning."
    />
    <meta name="ai:author" content="Radostin Cholakov" />
    <meta
      name="ai:expertise"
      content="Machine Learning, Large Language Models, Quantization, AI Research, Computer Science"
    />
    <meta name="ai:institution" content="Stanford University" />

    <link rel="stylesheet" type="text/css" href="stylesheet.css" />

    <!-- Firebase Analytics -->
    <script type="module">
      // Import the functions you need from the SDKs you need
      import { initializeApp } from "https://www.gstatic.com/firebasejs/12.1.0/firebase-app.js";
      import { getAnalytics } from "https://www.gstatic.com/firebasejs/12.1.0/firebase-analytics.js";
      // TODO: Add SDKs for Firebase products that you want to use
      // https://firebase.google.com/docs/web/setup#available-libraries

      // Your web app's Firebase configuration
      // For Firebase JS SDK v7.20.0 and later, measurementId is optional
      const firebaseConfig = {
        apiKey: "AIzaSyBLfLF2CNMG7-zlCLtvltryeFUPWwAo-Jw",
        authDomain: "radicho-com.firebaseapp.com",
        projectId: "radicho-com",
        storageBucket: "radicho-com.firebasestorage.app",
        messagingSenderId: "441608934287",
        appId: "1:441608934287:web:6882566dc3a55f164431fa",
        measurementId: "G-BVNRK2X2E0",
      };

      // Initialize Firebase
      const app = initializeApp(firebaseConfig);
      const analytics = getAnalytics(app);
    </script>
  </head>
  <body>
    <table
      style="
        width: 100%;
        max-width: 800px;
        border: 0px;
        border-spacing: 0px;
        border-collapse: separate;
        margin-right: auto;
        margin-left: auto;
      "
    >
      <tbody>
        <tr style="padding: 0px">
          <td style="padding: 0px">
            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr class="header-row" style="padding: 0px">
                  <td style="padding: 2.5%; width: 63%; vertical-align: middle">
                    <p class="name" style="text-align: center">
                      Radostin Cholakov
                    </p>
                    <p>
                      I'm a CS student at
                      <a href="https://www.stanford.edu/"
                        >Stanford University</a
                      >
                      focused on efficient large language models (quantization),
                      novel architectures (discrete diffusion), reasoning, and
                      representation learning.
                    </p>

                    <p>
                      From Barutin, Bulgaria. Since 4th grade some call me Radi
                      Cho :D
                    </p>
                    <p style="text-align: center">
                      <a href="mailto:radicho@stanford.edu">Email</a>
                      &nbsp;/&nbsp;
                      <a
                        href="https://scholar.google.com/citations?user=oYnCgH0AAAAJ&hl=en"
                        >Scholar</a
                      >
                      &nbsp;/&nbsp;
                      <a href="https://github.com/radi-cho">GitHub</a>
                      &nbsp;/&nbsp;
                      <a
                        href="https://www.linkedin.com/in/radostin-cholakov-bb4422146/"
                        >LinkedIn</a
                      >
                    </p>
                  </td>
                  <td style="padding: 2.5%; width: 37%; max-width: 37%">
                    <div class="profile-image-container"></div>
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td
                    style="padding: 16px; width: 100%; vertical-align: middle"
                  >
                    <h2>Papers</h2>
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px 10px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td style="padding: 16px; width: 20%; vertical-align: middle">
                    <img
                      src="images/imaginet-paper.png"
                      width="160"
                      alt="paper"
                    />
                  </td>
                  <td style="padding: 16px; width: 80%; vertical-align: middle">
                    <a href="https://arxiv.org/pdf/2407.20020"
                      ><span class="papertitle"
                        >ImagiNet: A Multi-Content Benchmark for Synthetic Image
                        Detection</span
                      ></a
                    ><br />
                    Delyan Boychev, <strong>Radostin Cholakov</strong><br />
                    AAAI 2025 Workshop on Datasets and Evaluators of AI Safety,
                    2025.
                    <a href="https://arxiv.org/pdf/2407.20020">paper</a> ·
                    <a
                      href="https://huggingface.co/datasets/delyanboychev/imaginet"
                      >dataset</a
                    >
                    <p>
                      ImagiNet provides 200K high-resolution examples across
                      photos, paintings, faces, and miscellaneous to improve
                      generalization of synthetic image detectors. A
                      SelfCon-trained ResNet‑50 sets strong baselines, reaching
                      up to 0.99 AUC and 86–95% balanced accuracy with
                      robustness to compression and resizing.
                    </p>
                  </td>
                </tr>
                <tr>
                  <td style="padding: 16px; width: 20%; vertical-align: middle">
                    <img src="images/flute-paper.png" width="160" alt="paper" />
                  </td>
                  <td style="padding: 16px; width: 80%; vertical-align: middle">
                    <a href="https://aclanthology.org/2024.findings-emnlp.724/"
                      ><span class="papertitle"
                        >Fast Matrix Multiplications for Lookup Table-Quantized
                        LLMs</span
                      ></a
                    ><br />
                    Han Guo, William Brandon,
                    <strong>Radostin Cholakov</strong>, Jonathan Ragan-Kelley,
                    Eric P. Xing, Yoon Kim<br />
                    <em>Findings of EMNLP</em>, 2024.
                    <a href="https://aclanthology.org/2024.findings-emnlp.724/"
                      >paper</a
                    >
                    · <a href="https://github.com/HanGuo97/flute">code</a> ·
                    <a
                      href="https://huggingface.co/collections/radi-cho/llama-flute-66af6b5af0bb52918931e519"
                      >models</a
                    >
                    <p>
                      We introduce FLUTE, a flexible LUT engine that fuses
                      dequantization and matmul for non-uniform low-bit weights,
                      reducing unpacking overhead and shared-memory contention.
                      At batch sizes &lt; 32 and group size 128, FLUTE is 2–4×
                      faster than strong GEMM baselines, yielding 1.5–2×
                      end-to-end throughput gains while maintaining competitive
                      accuracy.
                    </p>
                  </td>
                </tr>
                <tr>
                  <td style="padding: 16px; width: 20%; vertical-align: middle">
                    <img
                      src="images/distributional-paper.png"
                      width="160"
                      alt="paper"
                    />
                  </td>
                  <td style="padding: 16px; width: 80%; vertical-align: middle">
                    <a
                      href="https://www.cee.org/sites/default/files/rsi/Papers/Cholakov_Radostin.pdf"
                      ><span class="papertitle"
                        >Distributional Quantization of Large Language
                        Models</span
                      ></a
                    ><br />
                    <strong>Radostin Cholakov</strong>, Han Guo, Yoon Kim<br />
                    CEE RSI Distinguished Papers, 2023.
                    <a
                      href="https://www.cee.org/sites/default/files/rsi/Papers/Cholakov_Radostin.pdf"
                      >paper</a
                    >
                    <p>
                      We quantize LLM weights to 4 bits using block-wise
                      quantiles from fitted distributions (Gaussian, Beta,
                      Student’s t) and a numerical optimizer to minimize
                      reconstruction loss. The approach reduces error over prior
                      4-bit quantile methods at equal storage and achieves
                      state-of-the-art perplexity on LLaMA‑2 for WikiText‑2.
                    </p>
                  </td>
                </tr>
                <tr>
                  <td style="padding: 16px; width: 20%; vertical-align: middle">
                    <img src="images/rstod-paper.png" width="160" alt="paper" />
                  </td>
                  <td style="padding: 16px; width: 80%; vertical-align: middle">
                    <a href="https://aclanthology.org/2022.icnlsp-1.2/"
                      ><span class="papertitle"
                        >Efficient Task-Oriented Dialogue Systems with Response
                        Selection as an Auxiliary Task</span
                      ></a
                    ><br />
                    <strong>Radostin Cholakov</strong>, Todor Kolev<br />
                    <em>ICNLSP</em>, 2022.
                    <a href="https://aclanthology.org/2022.icnlsp-1.2/"
                      >paper</a
                    >
                    · <a href="https://github.com/radi-cho/RSTOD">code</a>
                    <p>
                      We add auxiliary response-selection tasks (distractor vs.
                      ground-truth and synthetic vs. ground-truth) to boost
                      task-oriented dialogue generation. On MultiWOZ 2.1 our
                      models reach state-of-the-art combined scores
                      (107.5/108.3) and outperform a baseline 3× larger, with
                      released code and checkpoints.
                    </p>
                  </td>
                </tr>
                <tr>
                  <td style="padding: 16px; width: 20%; vertical-align: middle">
                    <img
                      src="images/azbuki-paper.png"
                      width="160"
                      alt="paper"
                    />
                  </td>
                  <td style="padding: 16px; width: 80%; vertical-align: middle">
                    <a
                      href="https://illuxi-v3.s3.amazonaws.com/attachments/boot/9c068291750105dc575f6686e918fdad.pdf"
                      ><span class="papertitle"
                        >AzBuki.ML — Machine learning platform for NLP for the
                        Slavic languages</span
                      ></a
                    ><br />
                    <strong>Radostin Lozanov Cholakov</strong><br />
                    Expo Sciences Luxembourg, 2021.
                    <a
                      href="https://illuxi-v3.s3.amazonaws.com/attachments/boot/9c068291750105dc575f6686e918fdad.pdf"
                      >paper</a
                    >
                    <p>
                      A full-stack NLP platform featuring a 4M-row word-form
                      dictionary, POS tagger trained on BulTreeBank and BAS'
                      datasets, comma prediction BiRNN, and a 45K+ polarity
                      lexicon. Includes experimental embedding and LSTM models
                      for topic modeling, keyword extraction, and both
                      abstractive and extractive summarization.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td
                    style="padding: 16px; width: 100%; vertical-align: middle"
                  >
                    <h2>Preprints</h2>
                  </td>
                </tr>
              </tbody>
            </table>
            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px 10px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td style="padding: 16px; width: 20%; vertical-align: middle">
                    <img
                      src="images/gatedtabtransformer-paper.png"
                      width="160"
                      alt="preprint"
                    />
                  </td>
                  <td style="padding: 16px; width: 80%; vertical-align: middle">
                    <a href="https://arxiv.org/abs/2201.00199"
                      ><span class="papertitle"
                        >The GatedTabTransformer: An Enhanced Deep Learning
                        Architecture for Tabular Modeling</span
                      ></a
                    ><br />
                    <strong>Radostin Cholakov</strong>, Todor Kolev · 2022 ·
                    <a href="https://github.com/radi-cho/GatedTabTransformer"
                      >code</a
                    >
                    <p>
                      We extend TabTransformer with gated MLP-inspired linear
                      projections and improved training setups, yielding >1%
                      AUROC gains on three binary classification datasets. We
                      analyze activation choices and key hyperparameters,
                      offering practical guidance for robust tabular modeling.
                    </p>
                  </td>
                </tr>
                <tr>
                  <td style="padding: 16px; width: 20%; vertical-align: middle">
                    <img
                      src="images/future-paper.png"
                      width="160"
                      alt="preprint"
                    />
                  </td>
                  <td style="padding: 16px; width: 80%; vertical-align: middle">
                    <a href="https://arxiv.org/abs/2108.08224"
                      ><span class="papertitle"
                        >Transformers Predicting the Future: Applying Attention
                        in Next-frame and Time Series Forecasting</span
                      ></a
                    ><br />
                    <strong>Radostin Cholakov</strong>, Todor Kolev · 2021
                    <p>
                      We explore Transformer-style models for time series and
                      next-frame prediction as attention-based alternatives to
                      RNNs across diverse modalities. Our study examines anomaly
                      robustness, context handling, and memory use via
                      preprocessing, dimensionality reduction, and convolutional
                      encodings.
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td
                    style="padding: 16px; width: 100%; vertical-align: middle"
                  >
                    <h2>Projects</h2>
                  </td>
                </tr>
              </tbody>
            </table>
            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px 10px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td style="padding: 16px; width: 20%; vertical-align: middle">
                    <img src="images/cs336.png" width="160" alt="project" />
                  </td>
                  <td style="padding: 16px; width: 80%; vertical-align: middle">
                    <span class="papertitle"
                      ><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_">LM from Scratch (CS336)</a>:
                      <a href="https://github.com/radi-cho/cs336-a1">a1</a> ·
                      <a href="https://github.com/radi-cho/cs336-a2">a2</a> ·
                      <a href="/files/cs336-a3.pdf">a3</a> ·
                      <a href="https://github.com/radi-cho/cs336-a4">a4</a> ·
                      <a href="https://github.com/radi-cho/cs336-a5"
                        >a5</a
                      ></span
                    >, 2025<br />
                    <strong>Radostin Cholakov</strong><br />
                    An implementation of a GPT-style model from scratch, along
                    with tokenization tools, data cleaning pipelines, and
                    post-training scripts. Implemented for the
                    <a href="https://stanford-cs336.github.io/spring2025/"
                      >CS336 Spring 2025</a
                    >
                    course at Stanford by Profs. Percy Liang and Tatsu Hashimoto.
                  </td>
                </tr>
                <tr>
                  <td style="padding: 16px; width: 20%; vertical-align: middle">
                    <img src="images/llada-r1.png" width="160" alt="project" />
                  </td>
                  <td style="padding: 16px; width: 80%; vertical-align: middle">
                    <a href="https://x.com/cognition/status/1896738757782696009"
                      ><span class="papertitle">LLaDA r1</span></a
                    >, 2025<br />
                    <strong>Radostin Cholakov</strong>, Zeyneb Kaya, Joe Li,
                    Nicole Ma<br />
                    A remasking optimization head that learns which token
                    positions to remask at each step to improve sample quality
                    and convergence. Winner of the Mercor × Etched × Cognition
                    hackathon; see the announcement for details. Later, wrote a
                    <a
                      href="https://cs224r.stanford.edu/projects/pdfs/CS_224R_Final_Report__3_.pdf"
                      >CS224R class project</a
                    >
                    on the topic under the supervision of
                    <a href="https://minkaixu.com/">Minkai Xu</a> at Prof.
                    Stefano Ermon's lab.
                  </td>
                </tr>
                <tr>
                  <td style="padding: 16px; width: 20%; vertical-align: middle">
                    <img
                      src="images/imaginet-project.png"
                      width="160"
                      alt="project"
                    />
                  </td>
                  <td style="padding: 16px; width: 80%; vertical-align: middle">
                    <a
                      href="https://huggingface.co/datasets/delyanboychev/imaginet"
                      ><span class="papertitle">ImagiNet dataset</span></a
                    >, 2024<br />
                    Delyan Boychev, <strong>Radostin Cholakov</strong><br />
                    200K high-resolution examples across photos, paintings,
                    faces, and miscellaneous for robust synthetic image
                    detection. Two evaluation tracks (real vs. synthetic;
                    generator ID) with SelfCon baselines up to 0.99 AUC.
                    <a href="https://arxiv.org/pdf/2407.20020">paper</a>
                  </td>
                </tr>
                <tr>
                  <td style="padding: 16px; width: 20%; vertical-align: middle">
                    <img
                      src="images/flute-project.png"
                      width="160"
                      alt="project"
                    />
                  </td>
                  <td style="padding: 16px; width: 80%; vertical-align: middle">
                    <a href="https://github.com/HanGuo97/flute"
                      ><span class="papertitle">FLUTE</span></a
                    >, 2024<br />
                    Han Guo, <strong>Radostin Cholakov</strong><br />
                    Contributed improvements for LUT-quantized LLM matmuls,
                    including ≤4-bit non-uniform quantization and RTX 4090
                    support. Released associated LLaMA and Gemma models on
                    <a
                      href="https://huggingface.co/collections/radi-cho/llama-flute-66af6b5af0bb52918931e519"
                      >HF</a
                    >.
                  </td>
                </tr>
                <tr>
                  <td style="padding: 16px; width: 20%; vertical-align: middle">
                    <img
                      src="images/datasetGPT.png"
                      width="160"
                      alt="project"
                    />
                  </td>
                  <td style="padding: 16px; width: 80%; vertical-align: middle">
                    <a href="https://github.com/radi-cho/datasetGPT"
                      ><span class="papertitle">datasetGPT</span></a
                    >, 2023<br />
                    <strong>Radostin Cholakov</strong><br />
                    A command-line tool for programmatically generating textual
                    and conversational datasets with LLMs, with pluggable
                    prompting and schemas. Used to build downstream resources
                    such as
                    <a href="https://github.com/radi-cho/botbots">botbots</a>.
                  </td>
                </tr>
                <tr>
                  <td style="padding: 16px; width: 20%; vertical-align: middle">
                    <img src="images/tsai_logo.svg" width="160" alt="project" />
                  </td>
                  <td style="padding: 16px; width: 80%; vertical-align: middle">
                    <a
                      href="https://timeseriesai.github.io/tsai/models.gatedtabtransformer.html"
                      ><span class="papertitle"
                        >GatedTabTransformer in tsai</span
                      ></a
                    >
                    ·
                    <a
                      href="https://timeseriesai.github.io/tsai/models.gmlp.html"
                      >gMLP in tsai</a
                    >, 2022<br />
                    <strong>Radostin Cholakov</strong>, Todor Kolev<br />
                    Implemented state-of-the-art tabular and MLP architectures
                    in the `tsai` library (2022), with clean APIs and docs for
                    time series users. Contributions include model definitions,
                    training recipes, and examples.
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td
                    style="padding: 16px; width: 100%; vertical-align: middle"
                  >
                    <h2>Old projects</h2>
                  </td>
                </tr>
              </tbody>
            </table>
            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px 10px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td
                    style="
                      padding: 8px 16px;
                      width: 100%;
                      vertical-align: middle;
                    "
                  >
                    <ul
                      style="
                        margin: 0;
                        padding-left: 0;
                        list-style: none;
                        line-height: 1.75;
                      "
                    >
                      <li>
                        <a
                          href="https://github.com/radi-cho/noisy-sentences-dataset"
                          >noisy-sentences-dataset</a
                        >
                        - 550K sentences in 5 languages augmented with noise.
                      </li>
                      <li>
                        <a
                          href="https://x.com/radi_cho/status/1632299707962646531"
                          >diffground</a
                        >
                        - A mobile app to edit pictures with textual
                        instructions.
                      </li>
                      <li>
                        <a href="https://github.com/radi-cho/Diffusion-LM-TOD"
                          >Diffusion-LM-TOD</a
                        >
                        - Pre-train the diffusion model with task-oriented
                        conversational texts.
                      </li>
                      <li>
                        <a href="https://github.com/obecto/perper">perper</a> -
                        Stream-based, horizontally scalable framework for
                        asynchronous data processing.
                      </li>
                      <li>
                        <a href="https://github.com/radi-cho/tfjs-firebase"
                          >tfjs-firebase</a
                        >
                        - Train a TF model with data from Firestore and make
                        predictions in Cloud Functions.
                      </li>
                      <li>
                        <a href="https://github.com/RSG-Group/RSG-Chess-mobile"
                          >RSG-Chess-mobile</a
                        >
                        - Custom chess API and app with React Native.
                      </li>
                      <li>
                        <a href="https://github.com/radi-cho/star-rating"
                          >star-rating</a
                        >
                        - A Material Design web component.
                      </li>
                    </ul>
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td
                    style="padding: 16px; width: 100%; vertical-align: middle"
                  >
                    <h2>Technical blog & talks</h2>
                  </td>
                </tr>
              </tbody>
            </table>
            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px 4px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td
                    style="
                      padding: 8px 16px;
                      width: 100%;
                      vertical-align: middle;
                    "
                  >
                    <a
                      href="https://www.youtube.com/watch?v=j9kXuPFiNDA&list=PLitsCDUwEv0YcbWoopZGqpG5g61TgAgVx&index=5"
                      ><span class="papertitle"
                        >How AI sees the world: similarities and differences
                        with human perception? | DEV.BG All in One 2024</span
                      ></a
                    ><br />
                    <em class="blog-date">Sep 29, 2024</em> &mdash; Talk; video
                    in Bulgarian.
                  </td>
                </tr>
                <tr>
                  <td
                    style="
                      padding: 8px 16px;
                      width: 100%;
                      vertical-align: middle;
                    "
                  >
                    <a href="https://www.youtube.com/watch?v=R-LL8ve5Jfc"
                      ><span class="papertitle"
                        >Quantization Approaches for TensorFlow Models | ML
                        Study Jams TFUG Islamabad</span
                      ></a
                    ><br />
                    <em class="blog-date">Jun 8, 2024</em> &mdash; Talk on
                    weight-only and post-training quantization for TensorFlow
                    models;<br />
                    Also presented at DevFest Sofia: Cloud and Friends.
                  </td>
                </tr>
                <tr>
                  <td
                    style="
                      padding: 8px 16px;
                      width: 100%;
                      vertical-align: middle;
                    "
                  >
                    <a
                      href="https://medium.com/@radicho/the-power-of-long-contexts-gemini-1-5-pro-use-cases-in-research-1a9c163302d0"
                      ><span class="papertitle"
                        >The power of long contexts: Gemini 1.5 Pro use-cases in
                        research</span
                      ></a
                    ><br />
                    <em class="blog-date">Mar 24, 2024</em> &mdash; Demonstrates
                    how million-token context enables reasoning over entire
                    manuscripts and long-form media for research workflows.
                  </td>
                </tr>
                <tr>
                  <td
                    style="
                      padding: 8px 16px;
                      width: 100%;
                      vertical-align: middle;
                    "
                  >
                    <a
                      href="https://medium.com/@radicho/fine-tuning-the-multilingual-t5-model-from-huggingface-with-keras-f7f619ec5cfe"
                      ><span class="papertitle"
                        >Fine-tuning the multilingual T5 model from Hugging Face
                        with Keras</span
                      ></a
                    ><br />
                    <em class="blog-date">Feb 18, 2023</em> &mdash; Minimal
                    tutorial for fine-tuning mT5 for downstream tasks using the
                    Hugging Face ecosystem with TensorFlow/Keras.
                  </td>
                </tr>
                <tr>
                  <td
                    style="
                      padding: 8px 16px;
                      width: 100%;
                      vertical-align: middle;
                    "
                  >
                    <a
                      href="https://medium.com/@radicho/gmlp-what-it-is-and-how-to-use-it-in-practice-with-tensorflow-and-keras-76018800fea2"
                      ><span class="papertitle"
                        >Gated Multilayer Perceptron (gMLP): What it is and how
                        to use it?</span
                      ></a
                    ><br />
                    <em class="blog-date">Sep 10, 2022</em> &mdash; Explains how
                    gMLP works as a non-attention alternative to Transformers
                    and how to use it with TensorFlow/Keras.
                  </td>
                </tr>
                <tr>
                  <td
                    style="
                      padding: 8px 16px;
                      width: 100%;
                      vertical-align: middle;
                    "
                  >
                    <a
                      href="https://medium.com/@radicho/differentiable-discrete-sampling-in-tensorflow-da13b43a843"
                      ><span class="papertitle"
                        >Differentiable discrete sampling in TensorFlow</span
                      ></a
                    ><br />
                    <em class="blog-date">Aug 1, 2022</em> &mdash; A practical
                    guide to differentiable sampling for discrete variables used
                    downstream in neural networks.
                  </td>
                </tr>
                <tr>
                  <td
                    style="
                      padding: 8px 16px;
                      width: 100%;
                      vertical-align: middle;
                    "
                  >
                    <a href="https://www.youtube.com/watch?v=fsv0rty7QhU"
                      ><span class="papertitle"
                        >ML Engine - Machine Learning in the Cloud</span
                      ></a
                    ><br />
                    <em class="blog-date">Oct 4, 2018</em> &mdash; Video on
                    Google Cloud ML Engine on the
                    <a href="https://www.youtube.com/@Fireship">Fireship</a>
                    channel.
                  </td>
                </tr>
              </tbody>
            </table>

            <table
              style="
                width: 100%;
                border: 0px;
                border-spacing: 0px;
                border-collapse: separate;
                margin-right: auto;
                margin-left: auto;
              "
            >
              <tbody>
                <tr>
                  <td style="padding: 0px">
                    <br />
                    <p style="text-align: center; font-size: small">
                      Built from a simplified template (<a
                        href="https://github.com/jonbarron/jonbarron.github.io"
                        >source</a
                      >).
                    </p>
                  </td>
                </tr>
              </tbody>
            </table>
          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>
